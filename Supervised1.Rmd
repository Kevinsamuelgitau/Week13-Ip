#Week 13 Ip ##By Samuel Gitau

##Overview In this project, we will work with a Kenyan entrepreneur, who has created an online cryptography course and would want to advertise it on her blog. She currently targets audiences originating from various countries. In the past, she ran ads to advertise a related course on the same blog and collected data in the process. She would now like to employ your services as a Data Science Consultant to help her identify which individuals are most likely to click on her ads.

##Analytics question Our task is to help a Kenyan enterpreneur to predict the performance of her method to target individual to click on her ads to her crypotography course.

##Metrics for success Our project will be considered successful if we can accurately predict without overfitting/underfitting

##Experimental design Here is a step to step guide to our project

Loading and previewing the dataset
Studying the dataset properties
Data cleaning
EDA
Creating models

## Installing important packages----
```{r}
install.packages("dplyr")
library(dplyr)
install.packages("tidyr")
library(tidyr)
install.packages("ggplot2")
library(ggplot2)
install.packages("tibble")
library(tibble)
install.packages("Hmisc")
library('Hmisc')
install.packages("pander")
library(pander)
install.packages("corrplot")
library(corrplot)
install.packages("magrittr")
library(magrittr)
install.packages("sos")
library(sos)
findFn("select")
install.packages("data.table")
library(data.table)
install.packages("knitr")
library(knitr)
```
## Loading the dataset ----
```{r}
df <- read.csv("advertising.csv")
```


###Previewing the first 6 rows 
```{r}
head(df)
```

###Previewing the last 6 rows
```{r}
tail(df)
```


###Checking shape of dataset
```{r}
dim(df)
```


## Checking Information  our dataset----
###Checking number of columns 
```{r}
colnames(df)
```


###Convert to dataframe to a tibble for knit output
```{r}
df <- as.tibble(df)
df
```

###checking the datatypes of the dataframe
```{r}
??df
```


###Checking summary of our dataset
```{r}
summary(df)
```


## Cleaning the Dataset----
###Check for missing values
```{r}
colSums(is.na(df))
```


###checking for duplicated values
```{r}
duplicated_rows <- df[duplicated(df),]
duplicated_rows
```


###Checking for outliers
####Getting the numerical columns
```{r}
install.packages("MASS")
library(MASS)
install.packages("dplyr")
library(dplyr)
df_num <- (df %>% select(c("Daily.Time.Spent.on.Site", "Area.Income", "Age", "Daily.Internet.Usage")))
```

###using  Boxplots to check for outliers
```{r}
par(mfrow=c(1,4))
boxplot(df_num$Daily.Time.Spent.on.Site, xlab = "Daily.Time.Spent.on.Site")
boxplot(df_num$Area.Income, xlab = "Area.Income")
boxplot(df_num$Age, xlab = "Age")
boxplot(df_num$Daily.Internet.Usage, xlab = "Daily.Internet.Usage")
```


###looking at the outliers
```{r}
out <- list(boxplot.stats(df_num$Area.Income)$out)
out
```


###Removing outliers
```{r}
df_new <- subset(df, Area.Income > 19000)
dim(df_new)
```


###Clean character columns
####remove Whitespaces and convert character columns lower case
```{r}
df_new %>%
  summarise_if(is.character, tolower) %>% trimws()
```


## Univariate Analysis----
###continuous variables
```{r}
df_sum <-(df_new %>% summarise_if(is.numeric, summary))
df_sum <- data.frame(df_sum)
df_sum$Index <- c('Min', '1st QUN', 'Median','Mean', '3rd QUN', 'Max' )
rownames(df_sum) <- df_sum$Index
df_sum %>% select(- c('Index', 'Male', 'Clicked.on.Ad'))
```

###Checking for variance and standard deviation of the numerical columns
```{r}
df_num %>%
  summarise_if( is.numeric, var)
df_num %>%
  summarise_if( is.numeric, sd)
```
###plot histogram to check distribution on numerical values
```{r}
par(mfrow=c(4,1))
h <- hist(df_num$Daily.Time.Spent.on.Site, main="Daily.Time.Spent.on.Site", col = 'black')
xfit <- seq(min(df_new$Daily.Time.Spent.on.Site),max(df_new$Daily.Time.Spent.on.Site),length=40)
yfit<-dnorm(xfit,mean=mean(df_new$Daily.Time.Spent.on.Site),sd=sd(df_new$Daily.Time.Spent.on.Site))
yfit <- yfit*diff(h$mids[1:2])*length(df_new$Daily.Time.Spent.on.Site)
lines(xfit, yfit, col="blue", lwd=2)
h <- hist(df_new$Age, main="AGE DISTIBUTION", col = 'red')
xfit <- seq(min(df_new$Age),max(df_new$Age),length=40)
yfit<-dnorm(xfit,mean=mean(df_new$Age),sd=sd(df_new$Age))
yfit <- yfit*diff(h$mids[1:2])*length(df_new$Age)
lines(xfit, yfit, col="blue", lwd=2)
h <- hist(df_new$Area.Income, main="Area.Income Distribution", col = 'green')
xfit <- seq(min(df_new$Area.Income),max(df_new$Area.Income),length=40)
yfit<-dnorm(xfit,mean=mean(df_new$Area.Income),sd=sd(df_new$Area.Income))
yfit <- yfit*diff(h$mids[1:2])*length(df_new$Area.Income)
lines(xfit, yfit, col="blue", lwd=2)
h <- hist(df_new$Daily.Internet.Usage, main="Daily.Internet.Usage Distibution", col = 'blue')
xfit <- seq(min(df_new$Daily.Internet.Usage),max(df_new$Daily.Internet.Usage),length=40)
yfit<-dnorm(xfit,mean=mean(df_new$Daily.Internet.Usage),sd=sd(df_new$Daily.Internet.Usage))
yfit <- yfit*diff(h$mids[1:2])*length(df_new$Daily.Internet.Usage)
lines(xfit, yfit, col="black", lwd=2)
```


### Display count of our Advert clicks
```{r}
install.packages("ggplot2")
library(ggplot2)
ggplot(df_new) + geom_bar(aes(x = Clicked.on.Ad),fill = 'blue')
```
###Frequency  of countries that participated in the study
```{r}
df_grouped <- data.frame(table(df_new$Country))
sorted_by_county <- df_grouped[order(-df_grouped$Freq),][1:10,]
sorted_by_county
```


# Plot frequency  of countries that participated in the study
```{r}
ggplot(sorted_by_county, aes(x= Freq , y=Var1)) +
  geom_bar(stat="identity", fill="steelblue")+
  geom_text(aes(label=Freq), vjust=-0.3, size=3.5)+
  theme_minimal()
```


## Bivariate Anlysis----
###Group mean numerical columns by click on the ad
```{r}
df_new %>%
  group_by(Clicked.on.Ad) %>%
  summarise_if(is.numeric ,mean)
```


###Relationship between daily time spent on website and Clicking on the add
```{r}
ggplot(df_new) +
  geom_point(aes(x = Age, y= Daily.Time.Spent.on.Site ,color = Clicked.on.Ad))
```


###Relationship between Gender and Clicking on the Advert
```{r}
ggplot(df_new) +
  geom_point(aes(x = Male, y= Daily.Internet.Usage ,color = Clicked.on.Ad))
```


###Relationship between Income and Clicking on the ad
```{r}
ggplot(df_new) +
  geom_point(aes(x = Area.Income, y= Daily.Time.Spent.on.Site ,color = Clicked.on.Ad))
```


###Relationship between Timestamp and Clicking on the ad
```{r}
ggplot(df_new) +
  geom_point(aes(x = Timestamp , y= Daily.Internet.Usage ,color = Clicked.on.Ad))
```


###Correlation Matrix
###find correlation between columns
####use rcorr package
###Create a correlation plot
```{r}
install.packages("corrplot")
library(corrplot)
install.packages("Hmisc")
library('Hmisc')
df_num <- data.frame(select_if(df_new, is.numeric) )
res <- rcorr(as.matrix(df_num))
corr <- data.frame(res$r)
corr
corrplot(res$r, type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45)
```


###Get covariance between variables
```{r}
covv <- data.frame(cov(df_num))
covv
```
##Building models----
##KNN
#reading the database
```{r}
head(df_new)
```
#randomizing the rows
```{r}
random <- runif(150)
df_random <- df_new[order(random),]
```
#checking the first rows of the randomized dataset
```{r}
head(df_random)
```
#randomization
```{r}
normal <- function(x) (
  return( ((x - min(x)) /(max(x)-min(x))) )
)
df_nor <- as.data.frame(lapply(df_new[,c(2,3,4,5,8)], normal))
```
# Creating a random number equal 90% of total number of rows
```{r}
ran <- sample(1:nrow(df_new),0.9 * nrow(df_new))
```
# The training dataset extracted
```{r}
df_train <- df_random[ran,]
```
# The test dataset extracted
```{r}
df_test <- df_random[-ran,]
```
# Running the knn function
```{r}
library(class)
pr <- knn(dia_train,dia_test,cl=dia_target,k=20)
```
#svm(suport vector machine)
```{r}
install.packages("mlbench")
library(mlbench)
install.packages("caret")
library(caret)
intrain <- createDataPartition(y = df_new$Clicked.on.Ad, p= 0.7, list = FALSE)
training <- advertising[intrain,]
testing <- advertising[-intrain,]
```
```{r}
# We check the dimensions of out training dataframe and testing dataframe
# ---
# 
dim(training)
dim(testing)
```
```{r}
advertising[] <- lapply(advertising, function(x) as.numeric(as.character(x)))
```
#changing one column into factor 
```{r}
training[["Clicked.on.Ad"]] = factor(training[["Clicked.on.Ad"]])
```
```{r}
sum(!is.na(advertising$Clicked.on.Ad))
```
```{r}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
svm_Linear <- train(Clicked.on.Ad ~., data = training, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
```
```{r}
svm_Linear
```
```{r}
test_pred <- predict(svm_Linear, newdata = testing)
test_pred
```
```{r}
confusionMatrix(table(test_pred, testing$Clicked.on.Ad))
```
